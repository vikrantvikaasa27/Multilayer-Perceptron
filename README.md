# Multilayer-Perceptron

**Multi layer perceptron (MLP**) is a supplement of feed forward neural network. It consists of three types of layersâ€”the input layer, output layer and hidden layer.

Deep Learning algorithms use Artificial Neural Networks as their main structure.Deep Learning algorithms take in the dataset and learn its patterns, 
they learn how to represent the data with features they extract on their own. Then they combine different representations of the dataset, each one identifying a specific pattern or characteristic, into a more abstract,
high-level representation of the dataset.

A Multilayer Perceptron has input and output layers, and one or more hidden layers with many neurons stacked together. And while in the Perceptron the neuron must have an activation function that imposes a threshold, like ReLU or sigmoid, neurons in a Multilayer Perceptron can use any arbitrary activation function.

Multilayer Perceptron falls under the category of feedforward algorithms, because inputs are combined with the initial weights in a weighted sum and subjected to the activation function, just like in the Perceptron. But the difference is that each linear combination is propagated to the next layer.

Each layer is feeding the next one with the result of their computation, their internal representation of the data. This goes all the way through the hidden layers to the output layer.
